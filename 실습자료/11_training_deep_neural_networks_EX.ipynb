{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t0OVWA2FqF-"
   },
   "source": [
    "**Chapter 11 – Training Deep Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHurmLYGFqF_"
   },
   "source": [
    "Homework notebook\n",
    "\n",
    "### 학번:  \n",
    "\n",
    "### 이름:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT-ApNqDFqF_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyUhG5DGFqF_"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCuue5a6FqF_"
   },
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqC9FaqJFqF_"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "print(\"Python: \", sys.version_info)\n",
    "assert sys.version_info >= (3, 7)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.8 is required\n",
    "import tensorflow as tf\n",
    "print(\"TF version: \", tf.__version__)\n",
    "assert tf.__version__ >= \"2.8\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# GPU test\n",
    "print(\"GPU installed: \",tf.test.is_built_with_gpu_support())\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1kQsJZxFqGA"
   },
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CpeMwNGFqGA"
   },
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUi9RMlxFqGA"
   },
   "source": [
    "### Exercise 11.1  \n",
    "Build the same model as above with elu activation function, train the model, and compare the results. (The same training procedure as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdUOZ-9rFqGA"
   },
   "outputs": [],
   "source": [
    "# Ex.11.1a Model with elu activation\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model3 = keras.models.Sequential([\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMeOgoIQFqGB"
   },
   "source": [
    "#### Ex.11.1b Compare the results of loss, val_accuracy and training time for LeakyReLU, PReLU, SELU, and ELU  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51oxaPLkFqGB"
   },
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FG8hA6_FqGB"
   },
   "source": [
    "### Reusing a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zWOztxLFqGB"
   },
   "source": [
    "### Exercise 11.2  \n",
    "\"Reusing a Keras model\" 부분에서 Fashion MNIST의 class 5,6,7을 제외하고 7개의 class로 학습하고 class 5,6,7에 대해 300개의 학습데이터를 이용하여 단독 학습한 결과와 transfer leranng을 이용한 경우에 대해 비교하시오. 제외시키는 class가 3개로 늘어난 것 외에는 모두 위의 실습 코드와 동일하게 작성하시오. 결과를 비교하고 분석하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KIQAM0FFqGB"
   },
   "outputs": [],
   "source": [
    "# Ex.1.2a 7개 class 데이터를 이용한 학습\n",
    "def split_dataset(X, y):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iwOrdg5FqGB"
   },
   "outputs": [],
   "source": [
    "# Ex.11.2b 300개 데이터를 이용한 3개 class에 대한 학습\n",
    "model_C ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3ET3EKfFqGB"
   },
   "outputs": [],
   "source": [
    "#Ex. 11.2c Transfer learning using pregtrained layers\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_C_on_A ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrbzAZa4FqGB"
   },
   "source": [
    "#### Ex.11.2d 결과비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PRNauhPFqGB"
   },
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctWQwG73FqGB"
   },
   "source": [
    "### Exercise 11.3  \n",
    "Using Model6, cretae models from model6c to model6i for the above optimizers, train models and compare the results (training time and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfZrku0LFqGC"
   },
   "outputs": [],
   "source": [
    "# Ex.11.3a\n",
    "# Momentum\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model6c ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE9SSmMsFqGC"
   },
   "outputs": [],
   "source": [
    "# NAG\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model6d ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A28KuwDjFqGC"
   },
   "outputs": [],
   "source": [
    "# AdaGrad\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model6e ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKPjd6ygFqGC"
   },
   "outputs": [],
   "source": [
    "# RMSProp\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model6f ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rQGs3gBFqGC"
   },
   "outputs": [],
   "source": [
    "# Adam\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model6g ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_588yWddFqGC"
   },
   "outputs": [],
   "source": [
    "# Adamax\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model6h ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2H5AU3VFqGC"
   },
   "outputs": [],
   "source": [
    "# Nadamax\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model6i ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPRTh8eTFqGC"
   },
   "source": [
    "#### Ex.11.3b  \n",
    "Compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66Wbvrq_FqGC"
   },
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAqX6PZnFqGD"
   },
   "source": [
    "### Exercise 11.4  \n",
    "Train the model using the above keras.optimizers.schedules.ExponentialDecay learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KSSCwDrFqGD"
   },
   "outputs": [],
   "source": [
    "#EX.11.4\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "model13 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H36gTR7TFqGD"
   },
   "source": [
    "### Exercise 11.5  \n",
    "### 8. Deep Learning on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe0WDpRBFqGD"
   },
   "source": [
    "- 이번 문제는 컴퓨터 성능에 따라 학번의 학습이 몇시간이 걸릴 수 있으니 GPU가 설치된 고성능 PC나 colab에서 실행 권장  \n",
    "- 자율실습실(형남 1302/3호) 또는 차세대반도체학과 실습실(조만식 427호: 예약제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBeWLSCiFqGD"
   },
   "source": [
    "### a.\n",
    "히든 레이어 20개, 각 레이어 당 100개의 뉴런을 가진 DNN 모델을 구축하시요. 이 때 He initilization 과 ELU activation function 을 사용하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdHX6RKVFqGD"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Create a model: 20 hidden layers, 100 neurons each\n",
    "model9 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKXIeWagFqGD"
   },
   "source": [
    "### b.\n",
    "Nadam optimization 과 early stopping을 사용하여, 위에서 구축한 모델을 CIFAR10 dataset으로 학습하시오.\n",
    "\n",
    "CIFAR10 데이터셋 로드: keras.datasets.cifar10.load_data() CIFAR10 데이터셋: 총 10개의 class 존재 --> output feature 10인 softmax 필요\n",
    "\n",
    "참고)학습 때 모델 구조 또는 하이퍼파라미터 변경 시마다 적절한 learning rate 를 찾을 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odrZj30IFqGD"
   },
   "source": [
    "Let's add the output layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPd2dxXRFqGD"
   },
   "outputs": [],
   "source": [
    "# add output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzNn2e_-FqGD"
   },
   "source": [
    "Nadam optimizer, Learningrate:5e-5를 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAx1dIdZFqGE"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and compile model9\n",
    "optimizer ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SCIgOruFqGE"
   },
   "source": [
    "CIRAR10데이터셋 로드하기\n",
    "\n",
    "\n",
    "early stopping 추가 위해 validation set 설정(training set의 초반 5000개 이미지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BL5jLCI2FqGE"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset and generate train and valid dataset\n",
    "cifar10 = keras.datasets.cifar10\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyC62yUbFqGE"
   },
   "source": [
    "analysis of CIFAR 10 dataset. Refer the cells 14-23 of chap. 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPF9r2_kFqGE"
   },
   "outputs": [],
   "source": [
    "# Print shape and datatype of X_rain_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4YtWYybFqGE"
   },
   "outputs": [],
   "source": [
    "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\",\n",
    "               \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "# check y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33GVogbmFqGE"
   },
   "outputs": [],
   "source": [
    "# plot image of X_trian[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4zLwweVFqGE"
   },
   "outputs": [],
   "source": [
    "# Plot the first 40 images with labels. (4 x 10 format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSydxxVfFqGE"
   },
   "source": [
    "Now we can create the callbacks for tensorboard and early stopping and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYGldBfmFqGE"
   },
   "outputs": [],
   "source": [
    "# Define callbacks. Save checkpoint as my_cifar10_model.h5\n",
    "early_stopping_cb =\n",
    "model_checkpoint_cb ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWBJJfVAFqGF"
   },
   "outputs": [],
   "source": [
    "# Train model9: 100 epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-EH72bHFqGF"
   },
   "outputs": [],
   "source": [
    "# Tensoboard display\n",
    "\n",
    "# If you encounter an error, open a tab in your browser and type \"http://localhost:6006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx3quHE1FqGF"
   },
   "outputs": [],
   "source": [
    "# load the saved model9 to model10 and evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZs-_qQ-FqGF"
   },
   "source": [
    "### c.\n",
    "BatchNormalization 추가하여 Learning Curves 비교하기\n",
    "- 수렴 속도, 성능, 학습 속도 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXlntLifFqGF"
   },
   "source": [
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "* I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
    "* I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 10 epochs.\n",
    "* I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "yi7KK-i3FqGF"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Create model11 and add BatchNormalization layer before activation. Others are the same as above\n",
    "model11 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVA3_0CuFqGF"
   },
   "source": [
    "* *Is the model converging faster than before?* Much faster! The previous model took OO epochs to reach the lowest validation loss, while the new model with BN took OO epochs. That's more than twice as fast as the previous model. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "* *Does BN produce a better model?* Yes! The final model is also much better, with OO% accuracy instead of OO%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
    "* *How does BN affect training speed?* Although the model converged twice as fast, each epoch took about OOs instead of OOs, because of the extra computations required by the BN layers. So overall, although the number of epochs was reduced by OO%, the training time (wall time) was shortened by OO%. Which is still pretty significant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt7T0DZzFqGG"
   },
   "source": [
    "### d.\n",
    "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "g83HxBNLFqGG"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model12 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS4kNi4rFqGG"
   },
   "source": [
    "We get OO.O% accuracy, which is better than the original model, but not quite as good as the model using batch normalization. Moreover, it took OO epochs to reach the best model, which is much faster than both the original model and the BN model, plus each epoch took only OO seconds, just like the original model. So it's by far the fastest model to train (both in terms of epochs and wall time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYfyProvFqGG"
   },
   "source": [
    "### e.\n",
    "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFTkGXYyFqGG"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model13 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5I0uY0SnFqGG"
   },
   "source": [
    "The model reaches OO.O% accuracy on the validation set. That's very slightly worse than without dropout (OO.O%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYtNBPg7FqGG"
   },
   "source": [
    "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vu48XkpFqGG"
   },
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nihm2zy_FqGG"
   },
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98sTJoJaFqGG"
   },
   "outputs": [],
   "source": [
    "mc_model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f7QDCAGFqGH"
   },
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXA90sZGFqGH"
   },
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iev3F14jFqGH"
   },
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNiDZnWuFqGH"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FUYVrh0FqGH"
   },
   "source": [
    "We only get virtually no accuracy improvement in this case (from OO.O% to OO.O%).\n",
    "\n",
    "So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48YbOU8zFqGH"
   },
   "source": [
    "### f.\n",
    "*Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WH-DHBTJFqGH"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model14 ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Srw0oDBPFqGH"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model14, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deMozcM2FqGH"
   },
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGxam4AdFqGI"
   },
   "source": [
    "One cycle allowed us to train the model in just OO epochs, each taking only O seconds (thanks to the larger batch size). This is over O times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from OO.O% to OO.O%). The batch normalized model reaches a slightly better performance, but it's much slower to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LE_xdOgtFqGI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
